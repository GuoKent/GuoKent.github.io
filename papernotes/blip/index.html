

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg">
  <link rel="icon" href="/img/logo.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Kent">
  <meta name="keywords" content="">
  
    <meta name="description" content="BLIP背景BLIP(Bootstrapping Language-Image Pre-training)是一种统一的视觉语言理解与生成的预训练模型。其核心思想是通过 Bootstrapping 方法，利用 Captioner-Filter 机制生成高质量的文本标注，从而提高数据的质量和数量。BLiP 通过多模态混合结构 （Multimodal mixture of Encoder-Decoder">
<meta property="og:type" content="article">
<meta property="og:title" content="BLIP 系列">
<meta property="og:url" content="https://guokent.github.io/papernotes/blip/index.html">
<meta property="og:site_name" content="KentGuoK">
<meta property="og:description" content="BLIP背景BLIP(Bootstrapping Language-Image Pre-training)是一种统一的视觉语言理解与生成的预训练模型。其核心思想是通过 Bootstrapping 方法，利用 Captioner-Filter 机制生成高质量的文本标注，从而提高数据的质量和数量。BLiP 通过多模态混合结构 （Multimodal mixture of Encoder-Decoder">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://guokent.github.io/papernotes/blip/blip.png">
<meta property="og:image" content="https://guokent.github.io/papernotes/blip/blip2.png">
<meta property="og:image" content="https://guokent.github.io/papernotes/blip/blip2_2.png">
<meta property="article:published_time" content="2024-12-14T16:00:00.000Z">
<meta property="article:modified_time" content="2025-03-27T10:10:00.907Z">
<meta property="article:author" content="Kent">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="多模态">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://guokent.github.io/papernotes/blip/blip.png">
  
  
  
  <title>BLIP 系列 - KentGuoK</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"guokent.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha256-TThEtR+XalhWKkfF383YLOrI50NGNeIqrzS+q08afrY=" crossorigin="anonymous"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>HomePage</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/post.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="BLIP 系列"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-12-15 00:00" pubdate>
          2024年12月15日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          2.2k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          19 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">BLIP 系列</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="BLIP"><a href="#BLIP" class="headerlink" title="BLIP"></a>BLIP</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>BLIP(Bootstrapping Language-Image Pre-training)是一种统一的视觉语言理解与生成的预训练模型。其核心思想是通过 Bootstrapping 方法，利用 Captioner-Filter 机制生成高质量的文本标注，从而提高数据的质量和数量。BLiP 通过多模态混合结构 （Multimodal mixture of Encoder-Decoder， MED） 实现了视觉和语言的对齐与生成。</p>
<p><strong>论文：</strong> <a target="_blank" rel="noopener" href="https://proceedings.mlr.press/v162/li22n/li22n.pdf">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a></p>
<hr>
<p>在 BLIP 中，Bootstrapping 体现在 Captioner-Filter 机制中：</p>
<ul>
<li><strong>Captioner：</strong> 生成图像的文本标注。 </li>
<li><strong>Filter：</strong> 去除标注中的噪声，提升数据质量。 </li>
</ul>
<p>通过不断迭代生成和过滤，BLIP 能够从有限的标注数据中扩展出更多高质量的训练数据。</p>
<hr>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/papernotes/blip/blip.png" srcset="/img/loading.gif" lazyload title="模型结构"></p>
<p>BLIP 的核心是多模态混合结构(MED)，包括以下四个模块：</p>
<h3 id="Image-Encoder"><a href="#Image-Encoder" class="headerlink" title="Image Encoder"></a>Image Encoder</h3><ul>
<li>基于 Transformer 的 ViT 架构。将输入图像分割为多个 patch，并编码为一系列 Image Embedding </li>
<li>使用 <code>[CLS]</code> token 表示全局图像特征 </li>
<li>功能：提取图像特征，用于对比学习</li>
</ul>
<h3 id="Text-Encoder"><a href="#Text-Encoder" class="headerlink" title="Text Encoder"></a>Text Encoder</h3><ul>
<li>基于 BERT 架构</li>
<li>在输入文本开头添加 <code>[CLS]</code> token 以总结句子</li>
<li>功能：提取文本特征，用于对比学习</li>
</ul>
<h3 id="Image-grounded-Text-Encoder"><a href="#Image-grounded-Text-Encoder" class="headerlink" title="Image-grounded Text Encoder"></a>Image-grounded Text Encoder</h3><ul>
<li>在 Text Encoder 中添加 Cross-Attention 层，注入视觉信息</li>
<li>在输入文本开头添加 <code>[Encode]</code> token 以标识特定任务</li>
<li>功能：提取文本特征并与图像特征对齐</li>
</ul>
<h3 id="Image-grounded-Text-Decoder"><a href="#Image-grounded-Text-Decoder" class="headerlink" title="Image-grounded Text Decoder"></a>Image-grounded Text Decoder</h3><ul>
<li>将自注意力替换为因果自注意力(Causal Self-Attention)层</li>
<li>在输入文本开头和结尾分别添加 <code>[Decode]</code> token 和 <code>[EOS]</code> token，标识序列的开始和结束</li>
<li>功能：生成符合图像和文本特征的文本描述</li>
</ul>
<h2 id="预训练方法"><a href="#预训练方法" class="headerlink" title="预训练方法"></a>预训练方法</h2><h3 id="图文对比损失-Image-Text-Contrastive-Loss-ITC"><a href="#图文对比损失-Image-Text-Contrastive-Loss-ITC" class="headerlink" title="图文对比损失(Image-Text Contrastive Loss, ITC)"></a>图文对比损失(Image-Text Contrastive Loss, ITC)</h3><ol>
<li>目标：对齐图像和文本的特征空间</li>
<li>方法： <ul>
<li>最大化正样本图像-文本对的相似度</li>
<li>最小化负样本图像-文本对的相似度</li>
<li>使用动量编码器生成伪标签以辅助训练</li>
</ul>
</li>
<li>作用：用于训练 Image Encoder 和 Text Encoder</li>
</ol>
<h3 id="图文匹配损失-Image-Text-Matching-Loss-ITM"><a href="#图文匹配损失-Image-Text-Matching-Loss-ITM" class="headerlink" title="图文匹配损失(Image-Text Matching Loss, ITM)"></a>图文匹配损失(Image-Text Matching Loss, ITM)</h3><ol>
<li>目标：实现视觉和语言之间的细粒度对齐</li>
<li>方法：<ul>
<li>通过<strong>二分类任务</strong>预测图像-文本对是正样本还是负样本</li>
<li>使用 Hard negative mining 技术更好地捕捉负样本信息 </li>
</ul>
</li>
<li>作用： 用于训练 Image-grounded Text Encoder</li>
</ol>
<h3 id="语言建模损失-Language-Modeling-Loss-LM"><a href="#语言建模损失-Language-Modeling-Loss-LM" class="headerlink" title="语言建模损失(Language Modeling Loss, LM)"></a>语言建模损失(Language Modeling Loss, LM)</h3><ol>
<li>目标：生成图像的文本描述</li>
<li>方法： <ul>
<li>通过优化交叉熵损失函数，训练模型以自回归的方式最大化文本概率</li>
<li>使用 0.1 的标签平滑计算损失</li>
</ul>
</li>
<li>作用：用于训练 Image-grounded Text Decoder</li>
</ol>
<h2 id="CapFilt-机制"><a href="#CapFilt-机制" class="headerlink" title="CapFilt 机制"></a>CapFilt 机制</h2><p>由于高质量的人工标注数据(如 COCO 数据集)有限，BLIP 从网络中收集了大量图像-文本对作为训练数据。然而这些网络数据的文本通常包含噪声。为此，BLIP 提出了 Captioning and Filtering (CapFilt)机制 ：</p>
<h3 id="字幕器-Captioner"><a href="#字幕器-Captioner" class="headerlink" title="字幕器 Captioner"></a>字幕器 Captioner</h3><ul>
<li>功能： 基于 Image-grounded Text Decoder， 生成给定图像的文本描述</li>
<li>训练：在 COCO 数据集上使用 LM 损失函数进行微调</li>
<li>输出： 给定网络图片 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mi>W</mi></msub></mrow><annotation encoding="application/x-tex">I_W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，生成字幕 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">T_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</li>
</ul>
<h3 id="过滤器-Filter"><a href="#过滤器-Filter" class="headerlink" title="过滤器 Filter"></a>过滤器 Filter</h3><ul>
<li>功能： 基于 Image-grounded Text Encoder，去除文本噪声</li>
<li>训练：在 COCO 数据集上使用 ITC 和 ITM 损失函数进行微调</li>
<li>方法：通过比对文本和图像的匹配情况，删除原始 Web 文本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">T_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和合成文本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">T_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 中的噪声</li>
</ul>
<h3 id="数据集构建"><a href="#数据集构建" class="headerlink" title="数据集构建"></a>数据集构建</h3><p>将过滤后的图像-文本对与人工注释对结合，形成新的高质量数据集。使用该数据集重新训练 BLIP 模型。</p>
<h2 id="总训练流程"><a href="#总训练流程" class="headerlink" title="总训练流程"></a>总训练流程</h2><ol>
<li>初始训练：使用含有噪声的网络数据训练 BLIP</li>
<li>微调：在 COCO 数据集上微调 Captioner 和 Filter</li>
<li>数据过滤：使用 Filter 从原始网络文本和合成文本中去除噪声，得到干净数据</li>
<li>最终训练：使用干净数据重新训练 BLIP，得到高性能模型</li>
</ol>
<h1 id="BLIP2"><a href="#BLIP2" class="headerlink" title="BLIP2"></a>BLIP2</h1><p>与 BLIP 相比，BLIP-2 模型新引入 Querying Transformer(Q-Former)模块用于对图文进行对齐。</p>
<h2 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/papernotes/blip/blip2.png" srcset="/img/loading.gif" lazyload title="BLIP-2 结构"></p>
<p>BLIP-2由以下三个主要组件组成： </p>
<ul>
<li><strong>Image Encoder：</strong> 从输入图片中提取视觉特征。文中采用了两种不同的网络结构：CLIP 训练过的 ViT-L/14 和 EVA-CLIP 训练过的 ViT-g/14。 </li>
<li><strong>Large Language Model (LLM)：</strong> 大语言模型进行文本生成。<blockquote>
<p>文中尝试了两种不同的网络结构： decoder-based LLM 和 encoder-decoder-based LLM</p>
</blockquote>
</li>
<li><strong>Q-Former：</strong> 弥补视觉和语言两种模态间的差异，实现跨模态间的对齐。Q-Former 使用了一组可学习的查询向量 (Queries）来从冻结的 Image Encoder 中提取视觉特征，然后传入 LLM 供其生成文本。 Q-Former 的结构由 Image Transformer 和 Text Transformer 两个子模块构成， 它们<strong>共享相同的自注意力层</strong>。 <ol>
<li><strong>Image Transformer：</strong> 用于与冻结的图像编码器进行交互，从中提取一定数量的输出特征。 </li>
<li><strong>Text Transformer：</strong> 既可以作为文本编码器，也可以作为文本解码器。它创建一组可学习的 Queries 作为 mage Transformer 的输入，这些 Queries 在 Image Transformer 中通过自注意力层相互作用，并通过交叉注 意力层与冻结的图像特征进行交互。</li>
</ol>
</li>
</ul>
<h2 id="模型流程"><a href="#模型流程" class="headerlink" title="模型流程"></a>模型流程</h2><ol>
<li>Image Encoder 接收图像作为输入，输出图像的视觉特征</li>
<li>Q-Former 接收文本和 Image Encoder 输出的图像视觉特征，结合查询向量进行融合，学习与文本相近的视觉特征，输出 LLM 能够理解的视觉表示</li>
<li>LLM 模型接收 Q-Former 输出的视觉标识，生成对应文本</li>
</ol>
<h2 id="预训练方法-1"><a href="#预训练方法-1" class="headerlink" title="预训练方法"></a>预训练方法</h2><p>BLIP2采用两阶段预训练策略： </p>
<ol>
<li>表示学习阶段 （Representation Learning) </li>
<li>生成学习阶段 （Generative Learning ）</li>
</ol>
<h3 id="表示学习"><a href="#表示学习" class="headerlink" title="表示学习"></a>表示学习</h3><p>在表示学习阶段，Q-Former 被连接到冻结的 Image Encoder，训练集为图像-文本对。通过联合优化三个预训练目标，Q-Former 学习到高质量的跨模态对齐表示。为了控制 Image Transformer 和 Text Transformer 的交互方式，Q-Former 在 Query 和 Text 之间采用了不同的注意力掩码策略。</p>
<h4 id="图像-文本对比学习-ITC"><a href="#图像-文本对比学习-ITC" class="headerlink" title="图像-文本对比学习(ITC)"></a>图像-文本对比学习(ITC)</h4><p>ITC的自标是<strong>对齐图像嵌入和文本嵌入</strong>，最大化匹配图文对的相似度，同时最小化不匹配图文对的相似度。 </p>
<ul>
<li>计算来自 Image Transformer 的 Query 嵌入与来自 Text Transformer 的文本嵌入之间的相似度</li>
<li>为了避免信息泄漏，ITC 采用单模态自注意力掩码，禁止 Query 和 Text 之间的直接交互</li>
<li>Text Transformer 的文本嵌入是<code>[CLS]</code>标记的输出嵌入，而 Query 嵌入包含多个输出嵌入。计算每个 Query 嵌入 与文本嵌入的相似度，选择最高的一个作为图像-文本相似度</li>
</ul>
<h4 id="基于图像的文本生成-ITG"><a href="#基于图像的文本生成-ITG" class="headerlink" title="基于图像的文本生成(ITG)"></a>基于图像的文本生成(ITG)</h4><p>ITG的自标是在给定输入图像作为条件的情况下，<strong>训练 Q-Former生成文本</strong>，迫使 Query提取包含文本信息的视觉特征。 </p>
<ul>
<li>由于 Q-Former 的架构不允许冻结的图像编码器和文本标记之间的直接交互，生成文本所需的信息必须由 Query 提取，并通过自注意力层传递给文本标记</li>
<li>ITG 采用多模态因果注意力掩码(Causal Attention Mask)，允许 Query 相互关注，但不能关注 Text 标记。每个 Text 标记可以处理所有 Query 及其前面的 Text 标记</li>
<li>将<code>[CLS]</code>标记替换为新的<code>[DEC]</code>标记，作为第一个文本标记来指示解码任务</li>
</ul>
<h4 id="图像-文本匹配-ITM"><a href="#图像-文本匹配-ITM" class="headerlink" title="图像-文本匹配(ITM)"></a>图像-文本匹配(ITM)</h4><p>ITM 的目标是细粒度<strong>判断图文对是否匹配</strong>，从而增强模态对齐的局部一致性。 </p>
<ul>
<li>将 Image Transformer 输出的每个 Query 嵌入输入到一个二分类线性分类器中，获得对应的 logit</li>
<li>将所有 logit 平均，计算匹配分数。ITM 使用双向自注意力掩码，允许所有 Query 和 Text 之间相互关注</li>
</ul>
<h3 id="生成学习"><a href="#生成学习" class="headerlink" title="生成学习"></a>生成学习</h3><p><img src="/papernotes/blip/blip2_2.png" srcset="/img/loading.gif" lazyload title="生成学习阶段"></p>
<p>在生成学习阶段，Q-Former 被连接到冻结的 LLM，以利用 LLM 的语言生成能力。具体步骤如下： </p>
<ol>
<li><strong>特征投影：</strong> 使用全连接层将 Q-Former 输出的 Query 嵌入线性投影到与 LLM 文本嵌入相同的维度</li>
<li><strong>输入构造：</strong> 将投影后的 Query 嵌入添加到输入文本嵌入的前面</li>
<li><strong>生成任务：</strong> 由于 Q-Former 已经过预训练，能够提取包含语言信息的视觉表示，因此它可以作为信息瓶颈，将最有用的信息传递给LLM，同时过滤不相关的视觉信息，减轻 LLM学习视觉-语言对齐的负担</li>
</ol>
<p>BLIP2 试验了两种类型的 LLM：</p>
<ul>
<li><strong>基于 Decoder-only 的 LLM：</strong> 使用语言建模损失进行预训练，冻结的 LLM 根据 Q-Former 的视觉表示生成文本</li>
<li><strong>基于 Encoder-Decoder 的LLM：</strong> 使用前缀语言建模损失进行预训练，将文本分为前缀和后缀两部分。前缀文本与视觉表示连接作为 LLM 编码器的输入，后缀文本作为 LLM 解码器的生成目标</li>
</ul>
<hr>
<p>官方代码：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/salesforce/BLIP">BLIP</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/salesforce/LAVIS/blob/main/lavis/models/blip2_models/blip2_qformer.py">BLIP2</a></li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="category-chain-item">论文笔记</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" class="print-no-link">#大模型</a>
      
        <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" class="print-no-link">#多模态</a>
      
        <a href="/tags/%E8%AE%BA%E6%96%87/" class="print-no-link">#论文</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>BLIP 系列</div>
      <div>https://guokent.github.io/papernotes/blip/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Kent</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年12月15日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/papernotes/qwen/" title="Qwen 系列">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Qwen 系列</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/deeplearning/finetuning/" title="微调">
                        <span class="hidden-mobile">微调</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
