---
title: Deeplearning 基础
date: 2024-10-02 00:00:00
math: true
tags:
- 深度学习
- 算法
- 基础
categories:
- 算法杂记
alias:
- deeplearning/basic/
---

## 常用激活函数
### 1. sigmoid: 
$$
\sigma (x) = \frac{1}{1+e^{-x}}
$$
**优点：** 平滑且连续，适合二分类任务，具有明确的生物意义
**缺点：** 输入绝对值过大时有梯度消失问题，输出大多集中在 0 和 1，可能导致网络的权重更新不平衡。指数运算效率较慢

### 2. Tanh
$$
Tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$
**优点：** 输出均值为 0，收敛更快
**缺点：** 仍有梯度消失问题，计算慢

### 3. ReLU
$$
ReLU(x) = max(x, 0)
$$
**优点：** 计算简单且高效，无梯度消失问题。具有稀疏性，能提高训练效率
**缺点：** 对于 x<0，梯度恒为 0，神经元可能“死亡”，不再更新。输出非对称：输出范围是 $[0,+\infty)$，可能导致权重更新不平衡

### 4. Softmax
$$
f_i(x)=\frac{e^{x_i}}{\sum_j e^{x_i}}
$$
**优点：** 输出为概率分布，适合**多分类任务**。强化高分输出，抑制低分输出，具有良好的区分性。
**缺点：** 对异常值敏感，可能会梯度消失
> 通常在实际应用中，会将 softmax 的值减去最大值，目的是避免数值溢出。指数运算 e 当数值过大时会溢出。减去最大值不会影响最终结果，并且可以避免数值溢出

## 过拟合
过拟合指，模型在训练集上的效果远远大于测试集/验证集上的效果

### 缓解方法
1. 数据的角度：数据增强：图像数据就裁剪翻转等等、文本数据就是同义词替换
2. 正则化：L1/L2 正则化，在损失函数添加模型的参数，L2 是平方和，L1 是绝对值和。L2 正则化通常能避免权重过大，L1 正则化可以产生稀疏的模型（即很多权重为0，可以实现特征选择，计算资源需求小）。
3. dropout：随机对一些参数置 0
4. 早停：在验证集上性能下降就早停
5. 噪声注入

### 正则化有哪些？为什么要加入正则化
**正则化的目的**是通过限制模型的复杂度，防止过拟合
- L1 正则化： $L=Loss(y,\hat y)+ \lambda \sum_i |w_i|$，在损失函数中增加所有权重绝对值的惩罚项，会将某些权重 $w_i$ 收缩为 0，起到变量选择的作用。适用于**特征稀疏的场景**，容易导致某些特征被完全忽略。
- L2 正则化：$L=Loss(y, \hat y)+ \lambda \sum_i w_i^2 $，不会让权重完全为 0，而是倾向于将权重的值缩小。更适合多特征问题。
- Dropout：在每个训练批次中，以一定概率随机“屏蔽”一部分神经元，使其不参与前向和反向传播。防止神经网络中过度依赖某些特定的神经元

## LayerNorm & BatchNorm

## 损失函数
### MSE
MSE: Mean Squared Error
均方误差是指参数估计值与参数真值之差平方的期望值;
MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。
$$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

### RMSE
RMSE: Root Mean Squared Error
均方根误差:均方根误差是均方误差的算术平方根
$$ \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} $$

### MAE
MAE :Mean Absolute Error
平均绝对误差是绝对误差的平均值;
平均绝对误差能更好地反映预测值误差的实际情况.
$$ \text{MAE} = \frac{1}{N} \sum_{i=1}^N |(y_i - \hat{y}_i)| $$

### 交叉熵损失
熵：信息量关于概率分布 $P$ 的期望，$H(x)=-\sum_{i=0}^{N} P(X=x_i)logP(X=x_i)$
> 那些接近确定性的分布（输出几乎可以确定）具有较低的熵，那些接近均匀分布的概率分布具有较高的熵。故用交叉熵损失可让分类的分布更为确定/准确

单标签分类：$Loss=-\sum_{i=0}^{N}y_i \log \hat y_i$，其中 $y_i$ 为真实分布，$\hat y_i$ 为预测分布(输出的概率).
> 用一个例子来说明，在手写数字识别任务中，如果样本是数字“5”，那么真实分布应该为：[ 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ]，网络输出的分布为：[ 0.1, 0.1, 0, 0, 0, 0.7, 0, 0.1, 0, 0 ]

多标签分类(二元交叉熵损失)：$Loss=\frac{1}{n} \sum -y \log \hat y - (1-y) \log(1-\hat y)$
> 例如，图中有米饭和一些菜品，假设当前的多标签分类任务有三个标签：米饭(A)、南瓜(B)、青菜(C). 很明显，图中是没有青菜的，它的真实分布应该为: [ 1, 1, 0 ]. 假设网络输出的概率分布为: [ 0.8, 0.9, 0.1 ]
    $$
    \begin{aligned}
    Loss_A &= -1 \log 0.8 - (1-1) \log (1-0.8)\\
    Loss_B &= -1 \log 0.9 - (1-1) \log (1-0.9)\\
    Loss_C &= -0 \log 0.1 - (1-0) \log (1-0.1)\\
    Loss_{all} &= Loss_A + Loss_B + Loss_C
    \end{aligned}
    $$


## 逻辑回归
函数：$z=wX+b$

模型输出经过 sigmoid：$\hat y =\sigma(z)=\frac{1}{1+e^{-z}}$

用二元交叉熵损失：$L=\frac{1}{n} \sum -y \log \hat y - (1-y) \log(1-\hat y)$

梯度推导：
$$
\begin{aligned}
\frac{\partial L}{\partial w}=\frac{1}{n} X^T (\hat y - y)\\\\
\frac{\partial L}{\partial b}=\frac{1}{n} \sum_{i=1}^n (\hat y_i - y_i)
\end{aligned}
$$
梯度更新：$w^{(r+1)}=w^r-\alpha \frac{\partial L}{\partial w}$, $b^{(r+1)}=b^r-\alpha \frac{\partial L}{\partial b}$