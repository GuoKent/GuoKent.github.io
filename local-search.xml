<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Docker 操作</title>
    <link href="/developnotes/docker/"/>
    <url>/developnotes/docker/</url>
    
    <content type="html"><![CDATA[<h3 id="创建镜像"><a href="#创建镜像" class="headerlink" title="创建镜像"></a>创建镜像</h3><p>通常，创建 Docker 镜像的方法是通过 <code>Dockerfile</code> 文件。<code>Dockerfile</code> 是一个文本文件，包含了构建镜像所需的所有指令。</p><h5 id="基于-Ubuntu-创建-Python-环境"><a href="#基于-Ubuntu-创建-Python-环境" class="headerlink" title="基于 Ubuntu 创建 Python 环境"></a>基于 Ubuntu 创建 Python 环境</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用官方 Ubuntu 镜像作为基础镜像</span><br>FROM ubuntu:<span class="hljs-number">20.04</span><br><br><span class="hljs-comment"># 设置环境变量，防止在安装过程中出现交互式提示</span><br>ENV DEBIAN_FRONTEND=noninteractive<br><br><span class="hljs-comment"># 更新镜像并安装 Python 和 pip</span><br>RUN apt-get update &amp;&amp; apt-get install -y python3 python3-pip<br><br><span class="hljs-comment"># 设置工作目录</span><br>WORKDIR /app<br><br><span class="hljs-comment"># 将本地目录的代码复制到容器内</span><br>COPY . /app<br><br><span class="hljs-comment"># 安装 Python 依赖（假设有 requirements.txt 文件）</span><br>RUN pip3 install -r requirements.txt<br><br><span class="hljs-comment"># 默认运行命令</span><br>CMD [<span class="hljs-string">&quot;python3&quot;</span>, <span class="hljs-string">&quot;app.py&quot;</span>]<br></code></pre></td></tr></table></figure><h5 id="基于-Python-官方镜像"><a href="#基于-Python-官方镜像" class="headerlink" title="基于 Python 官方镜像"></a>基于 Python 官方镜像</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用官方 Python 3.9 镜像作为基础镜像</span><br>FROM python:<span class="hljs-number">3.9</span>-slim<br><br><span class="hljs-comment"># 设置工作目录</span><br>WORKDIR /app<br><br><span class="hljs-comment"># 复制当前目录内容到容器中的工作目录</span><br>COPY . /app<br><br><span class="hljs-comment"># 安装 Python 依赖</span><br>RUN pip install -r requirements.txt<br><br><span class="hljs-comment"># 运行 Python 应用</span><br>CMD [<span class="hljs-string">&quot;python&quot;</span>, <span class="hljs-string">&quot;app.py&quot;</span>]<br></code></pre></td></tr></table></figure><p>在包含 <code>Dockerfile</code> 文件的目录中运行以下命令来构建镜像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker build -t mylocalenv:latest .<br></code></pre></td></tr></table></figure><ul><li><code>-t mylocalenv:latest</code> 用于为镜像指定一个标签（这里是 <code>mylocalenv</code>）。</li><li><code>.</code> 表示当前目录是构建上下文。</li></ul><h3 id="拉取现有镜像"><a href="#拉取现有镜像" class="headerlink" title="拉取现有镜像"></a>拉取现有镜像</h3><p><code>docker pull [OPTIONS] NAME[:TAG|@DIGEST]</code>  // <strong>拉取镜像(下载)</strong></p><ul><li>NAME: 镜像名称，通常包含注册表地址（如 <a href="http://docker.io/library/ubuntu%EF%BC%89">docker.io/library/ubuntu）</a></li><li>TAG(可选)：镜像标签</li><li>DIGEST(可选)：镜像SHA256摘要</li></ul><p>例如：<code>docker pull continuumio/anaconda3</code> 从网上拉最常用的 Anaconda3 镜像</p><h3 id="启动容器并进入环境"><a href="#启动容器并进入环境" class="headerlink" title="启动容器并进入环境"></a>启动容器并进入环境</h3><p>常用指令：<code>docker run -it -v /mnt:/workspace/data my_env bash</code> </p><ul><li><code>-v</code> 代表挂在本地盘到容器中，让容器能访问本地磁盘</li><li><code>/mnt</code> 是本地磁盘路径，<code>/workspace/data</code> 是容器内部路径</li><li><code>bash</code> 代表我们希望用bash，看情况而加，若报错则不加</li><li><code>my_env</code> 是镜像名字</li></ul><h3 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h3><p><code>docker exec -it mycontainer bash</code><br><code>docker images</code> 列出镜像列表<br><code>docker ps -a</code> 列出所有容器，<code>-a</code> 代表所有<br><code>docker start mycontainer</code> 启动容器<br><code>docker stop mycontainer</code> 停止容器<br><code>docker kil mycontainer</code> 杀死容器<br><code>docker rm mycontainer</code> 删除容器<br><code>docker rmi myimage</code> 删除镜像（需删除关联容器）<br><code>Ctrl + D</code> 退出容器</p><h3 id="保存容器为新镜像"><a href="#保存容器为新镜像" class="headerlink" title="保存容器为新镜像"></a>保存容器为新镜像</h3><p><code>docker commit -a &#39;author&#39; -m &#39;instruction&#39; mycontainer new_image</code> 保存为新镜像<br><code>docker save -o tar_name.tar image_name</code> 将镜像保存为压缩包<br><code>docker load -i tar_name.tar</code> 读取压缩包镜像，然后用 <code>docker images</code> 就能看到一个新镜像</p><h3 id="本地-文件-环境-传入容器"><a href="#本地-文件-环境-传入容器" class="headerlink" title="本地 文件/环境 传入容器"></a>本地 文件/环境 传入容器</h3><p><code>docker cp /home/b/miniconda3/envs/py39 mycontainer:/opt/conda/envs</code><br>将本地的<code>py39</code>环境复制到容器<code>mycontainer</code>中</p>]]></content>
    
    
    <categories>
      
      <category>开发笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开发</tag>
      
      <tag>docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch 分布式训练</title>
    <link href="/developnotes/distribute/"/>
    <url>/developnotes/distribute/</url>
    
    <content type="html"><![CDATA[<h2 id="手动配置分布式训练"><a href="#手动配置分布式训练" class="headerlink" title="手动配置分布式训练"></a>手动配置分布式训练</h2><p>该方法自定义程度化较高，但环境等需自己配置，代码写起来较繁杂</p><h3 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.nn.parallel <span class="hljs-keyword">import</span> DistributedDataParallel <span class="hljs-keyword">as</span> DDP<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_distributed</span>(<span class="hljs-params">rank, world_size</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    初始化分布式环境</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    os.environ[<span class="hljs-string">&quot;MASTER_ADDR&quot;</span>] = <span class="hljs-string">&quot;127.0.0.1&quot;</span>  <span class="hljs-comment"># 本机地址</span><br>    os.environ[<span class="hljs-string">&quot;MASTER_PORT&quot;</span>] = <span class="hljs-string">&quot;29946&quot;</span>  <span class="hljs-comment"># 端口号，任取一个空端口就行</span><br>    dist.init_process_group(<br>        backend=<span class="hljs-string">&#x27;nccl&#x27;</span>,  <span class="hljs-comment"># NCCL 是 GPU 上分布式训练的推荐后端</span><br>        init_method=<span class="hljs-string">&#x27;env://&#x27;</span>,  <span class="hljs-comment"># 使用环境变量初始化</span><br>        rank=rank,<br>        world_size=world_size<br>    )<br>    torch.cuda.set_device(rank)  <span class="hljs-comment"># 将当前进程绑定到 rank 对应的 GPU</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_model</span>(<span class="hljs-params">model, rank</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    将 torch.model 放入分布式模型中</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># model = model.to(rank)  # 将模型移动到 rank 对应的 GPU</span><br>    executor = model._nn_executor.model.to(rank)  <span class="hljs-comment"># 将模型中torch.model部分放入gpu</span><br>    ddp_model = DDP(executor, device_ids=[rank])  <span class="hljs-comment"># 使用 DDP 包装模型(torch.model类)</span><br>    <span class="hljs-keyword">return</span> ddp_model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_data</span>(<span class="hljs-params">dataset, rank, world_size, batch_size, collate_fn=<span class="hljs-literal">None</span>, num_workers=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    数据并行, 数据转为分布式数据</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=<span class="hljs-literal">False</span>)<br>    dataloader = DataLoader(dataset, <br>                            batch_size=batch_size, <br>                            sampler=sampler, <br>                            collate_fn=collate_fn, <br>                            num_workers=num_workers,<br>                            pin_memory=<span class="hljs-literal">True</span>,<br>                            prefetch_factor=<span class="hljs-number">8</span>,<br>                            persistent_workers=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> dataloader<br></code></pre></td></tr></table></figure><ul><li><code>num_workers</code> 线程数，一般去cpu线程数的 1/2，或取gpu数量。但取多了会占大量内存</li><li><code>pin_memory</code> 固定数据在内存中的地址，可加快读取速度，但可能会导致占用内存大</li><li><code>prefetch_factor</code> 预先取多少个batch到内存中，默认为2，调大可加快读取速度</li><li><code>persistent_workers</code> 每次迭代结束是否保留进程，默认为False，可加快读写速度</li><li><code>collate_fn</code> 默认将 <code>[(data 1, label 1), (data 2, label 2), …]</code> 转化为<code>[[data 1, data 2, ...], [label 1, label 2, ...]]</code> 若要自定义<code>collate_fn</code> 则需自行转换</li></ul><h3 id="执行函数"><a href="#执行函数" class="headerlink" title="执行函数"></a>执行函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>(<span class="hljs-params">model, dataset</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    主运行函数 (主进程)</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    world_size = torch.cuda.device_count()<br>    mp.spawn(<br>        inference,  <span class="hljs-comment"># 传入推理/训练函数, 默认会把第一个rank参数传入</span><br>        args=(world_size, model, dataset),  <span class="hljs-comment"># 推理/训练函数的其他参数</span><br>        nprocs=world_size,<br>        join=<span class="hljs-literal">True</span><br>    )<br>    ...<br> <br> <span class="hljs-keyword">def</span> <span class="hljs-title function_">inference</span>(<span class="hljs-params">rank, world_size, model, dataset</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    推理/训练函数 (每个gpu执行的函数)</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        rank: 当前 gpu 对应的 rank</span><br><span class="hljs-string">        world_size: gpu 总数</span><br><span class="hljs-string">        model: torch.model</span><br><span class="hljs-string">        dataset: torch.dataset</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># 初始化分布式环境</span><br>    setup_distributed(rank, world_size)<br>    <span class="hljs-comment"># 准备模型</span><br>    ddp_model: DDP = prepare_model(model, rank)<br>    <span class="hljs-comment"># 准备数据</span><br>    dataloader = prepare_data(dataset, rank, world_size, batch_size=BATCH_SIZE, collate_fn=<span class="hljs-literal">None</span>, num_workers=NUM_WORKERS)<br>    <span class="hljs-comment"># 推理</span><br>    ddp_model.<span class="hljs-built_in">eval</span>()<br>    fail_batch = <span class="hljs-number">0</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Begin inference, model rank: <span class="hljs-subst">&#123;rank&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> tqdm(dataloader, total=<span class="hljs-built_in">len</span>(dataloader)):<br></code></pre></td></tr></table></figure><h3 id="结果写入和保存"><a href="#结果写入和保存" class="headerlink" title="结果写入和保存"></a>结果写入和保存</h3><p>由于是多卡推理/训练，涉及到文件读写冲突问题，因此需要制定策略防止文件写冲突</p><ul><li>每张卡各自写到自己的文件，整个训练/推理过程结束完最后再合并（推荐，并行写入更快）</li><li>只有一个结果文件，每张卡轮流写入（进程写入结果文件要排队，降低效率）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> fcntl<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">write_result_to_file</span>(<span class="hljs-params">batch, results, rank</span>):<br><span class="hljs-string">&#x27;&#x27;&#x27; 每个线程的结果写入临时文件, 或者单独写入一个文件&#x27;&#x27;&#x27;</span><br>sim_temp_path = <span class="hljs-string">f&quot;./res/temp/results_rank_<span class="hljs-subst">&#123;rank&#125;</span>.txt&quot;</span><br>    is_header = <span class="hljs-literal">False</span> <span class="hljs-keyword">if</span> os.path.exists(sim_temp_path) <span class="hljs-keyword">else</span> <span class="hljs-literal">True</span><br>    <br>    <span class="hljs-comment"># ... 结果处理，得到写入文件的格式</span><br>    new_df = pd.DataFrame(new_rows)  <span class="hljs-comment"># 要写入文件的格式</span><br>    <br>    <span class="hljs-comment"># 写入临时文件</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(sim_temp_path, <span class="hljs-string">&#x27;a&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-comment"># 独占锁</span><br>        fcntl.flock(f, fcntl.LOCK_EX)<br>        <span class="hljs-keyword">try</span>:<br>            new_df.to_csv(f, sep=<span class="hljs-string">&#x27;\t&#x27;</span>, index=<span class="hljs-literal">False</span>, header=is_header, mode=<span class="hljs-string">&#x27;a&#x27;</span>)<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Raise Error: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br>        <span class="hljs-keyword">finally</span>:<br>            <span class="hljs-comment"># 解锁</span><br>            fcntl.flock(f, fcntl.LOCK_UN)<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    当 num_workers 设定大于gpu数量时，一个gpu可能会执行多个线程的任务。</span><br><span class="hljs-string">    当线程1再cuda:0上执行完，然后执行写入临时文件。若线程1的写文件还没执行完，线程2也在</span><br><span class="hljs-string">    cuda:0上执行完，也开始写入临时文件，就会发生冲突</span><br><span class="hljs-string">    因此需要一个互斥锁来保证两者的写操作冲突</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br> <br> <span class="hljs-keyword">def</span> <span class="hljs-title function_">merge_results_from_files</span>(<span class="hljs-params">world_size, save_path</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    将每个gpu的结果合并到一起</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    is_header = <span class="hljs-literal">False</span> <span class="hljs-keyword">if</span> os.path.exists(save_path) <span class="hljs-keyword">else</span> <span class="hljs-literal">True</span><br>    <span class="hljs-comment"># 合并每个rank的结果</span><br>    <span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(world_size):<br>        sim_temp_path = <span class="hljs-string">f&quot;./res/temp/results_rank_<span class="hljs-subst">&#123;rank&#125;</span>.txt&quot;</span><br>        rank_file = pd.read_table(sim_temp_path, sep=<span class="hljs-string">&#x27;\t&#x27;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>        rank_file.to_csv(save_path, sep=<span class="hljs-string">&#x27;\t&#x27;</span>, index=<span class="hljs-literal">False</span>, header=is_header, mode=<span class="hljs-string">&#x27;a&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Finish merge file to: <span class="hljs-subst">&#123;save_path&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">delete_temp_file</span>():<br>    <span class="hljs-string">&#x27;&#x27;&#x27;删除临时文件(可选)&#x27;&#x27;&#x27;</span><br>    temp_folder = <span class="hljs-string">&quot;./res/temp/&quot;</span><br>    temp_file_names = [<span class="hljs-string">f&quot;results_rank_<span class="hljs-subst">&#123;rank&#125;</span>.txt&quot;</span> <span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(torch.cuda.device_count())]<br>    <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> temp_file_names:<br>        file_path = os.path.join(temp_folder, file_name)<br>        <span class="hljs-keyword">if</span> os.path.isfile(file_path):<br>            <span class="hljs-keyword">try</span>:<br>                os.remove(file_path)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Delete file: <span class="hljs-subst">&#123;file_path&#125;</span> successfully&quot;</span>)<br>            <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Raise Error when delete <span class="hljs-subst">&#123;file_path&#125;</span>: <span class="hljs-subst">&#123;e&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></li></ul><h2 id="自动配置分布式训练"><a href="#自动配置分布式训练" class="headerlink" title="自动配置分布式训练"></a>自动配置分布式训练</h2><p>另一种分布式训练写法，就是使用torchrun来执行python文件。运行的主函数只需关注每一个gpu的代码怎么运行即可，torchrun会自动分配环境给每一gpu。该方法只需考虑每个 gpu 对应的执行函数即可，代码写起来较为简单，也无需考虑文件互斥的问题，运行时直接 torchrun 自动执行分布式环境</p><blockquote><p>一个典型的例子：<a href="https://github.com/OFA-Sys/Chinese-CLIP/blob/master/cn_clip/training/main.py">CLIP</a></p></blockquote><h3 id="执行函数-1"><a href="#执行函数-1" class="headerlink" title="执行函数"></a>执行函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.parallel.DistributedDataParallel <span class="hljs-keyword">as</span> DistributedDataParallel<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    每个 gpu 的执行函数</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    args = parse_args()<br>    <br>    <span class="hljs-comment"># 查看当前gpu是哪个rank</span><br>    args.local_device_rank = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>])<br>    torch.cuda.set_device(args.local_device_rank)<br>    args.device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>, args.local_device_rank)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Init process group...&quot;</span>)  <span class="hljs-comment"># 准备环境</span><br>    dist.init_process_group(backend=<span class="hljs-string">&quot;nccl&quot;</span>, init_method=<span class="hljs-string">&#x27;env://&#x27;</span>)<br>    args.rank = dist.get_rank()<br>    args.world_size = dist.get_world_size()<br>    <br>    <span class="hljs-comment"># 准备模型</span><br>    model = MyModel()  <span class="hljs-comment"># 定义自己的模型</span><br>    model = DistributedDataParallel(model, ...)  <span class="hljs-comment"># 放入分布式模型里</span><br>    <br>    <span class="hljs-comment"># 准备数据集</span><br>    dataset = MyDataset()<br>    sampler = DistributedSampler(dataset, shuffle=<span class="hljs-literal">True</span>, seed=args.seed)<br>    dataloader = DataLoader(<br>        dataset,<br>        batch_size=batch_size,<br>        pin_memory=<span class="hljs-literal">False</span>,<br>        num_workers=args.num_workers <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> args.valid_num_workers,<br>        sampler=sampler,<br>        collate_fn=collate_fn<br>    )<br>    <br>    <span class="hljs-comment"># 优化器</span><br>    optimizer = optim.AdamW(...)<br>    <br>    <span class="hljs-comment"># 训练</span><br>    train(model, ...)<br> <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>main()<br></code></pre></td></tr></table></figure><h3 id="执行脚本"><a href="#执行脚本" class="headerlink" title="执行脚本"></a>执行脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">默认8卡全部</span><br>nohup torchrun --nproc_per_node=8 --master_port=29500 train_cnclip.py --max-epochs 10 --use-augment &gt; ./logs/train.log 2&gt;&amp;1 &amp;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">指定其中几张卡(默认按顺序取)</span><br>nohup CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 --master_port=29500 train_cnclip.py --max-epochs 10 --use-augment &gt; ./logs/train.log 2&gt;&amp;1 &amp;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">默认按顺序取前4张卡</span><br>nohup torchrun --nproc_per_node=4 --master_port=29500 train_cnclip.py --max-epochs 10 --use-augment &gt; ./logs/train.log 2&gt;&amp;1 &amp;<br><br><br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>开发笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>开发</tag>
      
      <tag>Pytorch</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python</title>
    <link href="/developnotes/python/"/>
    <url>/developnotes/python/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>开发笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开发</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git 笔记</title>
    <link href="/developnotes/git/"/>
    <url>/developnotes/git/</url>
    
    <content type="html"><![CDATA[<h3 id="git-clone"><a href="#git-clone" class="headerlink" title="git clone"></a>git clone</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone：<br>git clone http://xxxx.git  // 默认master分支<br>git clone -b &lt;branch_name&gt; http://xxxx.git  // clone指定分支<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">clone</span> 特定版本/分支</span><br>git clone -branch &lt;branch-name&gt; &lt;repo-address&gt;  // 克隆特定分支<br>git tag  // 列出全部tag(先clone全部仓库)<br>git checkout &lt;tag&gt;  // 切换到指定tag(版本)<br></code></pre></td></tr></table></figure><h3 id="连接仓库-remote"><a href="#连接仓库-remote" class="headerlink" title="连接仓库 remote"></a>连接仓库 remote</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">git remote -v  // 查看当前连接<br>git remote remove origin  // 删除现有连接<br>git remote add origin 你的远程库地址  // 把本地库与远程库关联<br></code></pre></td></tr></table></figure><h3 id="git-提交"><a href="#git-提交" class="headerlink" title="git 提交"></a>git 提交</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">第一次提交</span><br>git init   // 初始化版本库<br>git add .   // 添加文件到版本库（只是添加到缓存区），.代表添加文件夹下所有文件<br>git add -f &lt;路径&gt;  // 添加被忽略文件<br>git commit -m &quot;first commit&quot; // 把添加的文件提交到版本库，并填写提交备注<br>git push -u origin master    // 第一次推送时<br>git push  // 第一次推送后，直接使用该命令即可推送修改<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">后续提交</span><br>git add .<br>git commit -m &quot;message&quot;<br>git push origin master<br>git push origin &lt;new-brance&gt;  // master无法访问时用新分支 new-branch<br></code></pre></td></tr></table></figure><h3 id="远程更新代码"><a href="#远程更新代码" class="headerlink" title="远程更新代码"></a>远程更新代码</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git pull origin<br>git pull origin &lt;branch&gt;  // 初次, 指定分支<br></code></pre></td></tr></table></figure><h3 id="分支操作"><a href="#分支操作" class="headerlink" title="分支操作"></a>分支操作</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">git branch --list  // 查看分支<br>git branch -m new-name  // 分支重命名<br>git checkout &lt;branch-name&gt;  // 切换到分支<br>git checkout -b &lt;branch-name&gt; // 新建分支<br>git merge target_branch  // 将目标分支合并到当前分支<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>开发笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开发</tag>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BLIP</title>
    <link href="/papernotes/blip/"/>
    <url>/papernotes/blip/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>论文笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>多模态</tag>
      
      <tag>大模型</tag>
      
      <tag>论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Attention 算法</title>
    <link href="/deeplearning/attention/"/>
    <url>/deeplearning/attention/</url>
    
    <content type="html"><![CDATA[<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><script type="math/tex; mode=display">\begin{aligned}& Q = X \cdot W_Q, \quad K = X \cdot W_K, \quad V = X \cdot W_V \\\\& \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \\\\& X.\text{shape}: [B, T, D], \quad W.\text{shape}: [D, D], \quad d_k = D\end{aligned}</script><h3 id="理论解释为什么-Attention-有效"><a href="#理论解释为什么-Attention-有效" class="headerlink" title="理论解释为什么 Attention 有效"></a>理论解释为什么 Attention 有效</h3><blockquote><p>待更新</p></blockquote><h3 id="为什么要-scaling？"><a href="#为什么要-scaling？" class="headerlink" title="为什么要 scaling？"></a>为什么要 scaling？</h3><p>scaling 目的是解决数值稳定性问题，从而提高训练的效率和性能。当 Q，K 的维度 $d_k$ 很大时， $q_i,k_i$ 的点积值可能变得很大。点积值越大，输入到 softmax 函数中的数值范围越广，可能会导致以下问题：</p><ul><li><strong>softmax 的梯度变得极小</strong>: softmax 函数对大数值非常敏感，极大值会导致其他位置的权重几乎为 0，从而产生数值不稳定性。</li><li><strong>模型训练变得困难</strong>: 梯度消失问题会使得模型难以学习。</li></ul><h3 id="为什么是-d-k-而不是其他数？"><a href="#为什么是-d-k-而不是其他数？" class="headerlink" title="为什么是 $d_k$ 而不是其他数？"></a>为什么是 $d_k$ 而不是其他数？</h3><p>对于输入特征 $X$，其元素通常服从均值为 0、方差为 1 的标准正态分布。经过点积计算 $QK^T$ 后，由于 $q_i,k_i$ 的点积是 $d_k$ 个独立随机变量的和，所以方差会变为 $d_k$，除以 $\sqrt{d_k}$ 可以让方差重新变为 1。</p><ul><li>如果直接除以 $d_k$，方差变为 $1/d_k$，分布过于集中，让 softmax 的值趋于均匀分布，会弱化注意力机制的效果</li><li>如果除以 $\sqrt[3]{d_k}$，会让方差仍然较大，可能会导致数值不稳定和训练困难</li></ul><h3 id="其余-scale-处理"><a href="#其余-scale-处理" class="headerlink" title="其余 scale 处理"></a>其余 scale 处理</h3><ol><li><p>归一化 Attention 分数，放缩到[0,1]范围</p><script type="math/tex; mode=display"> score=\frac{QK^T}{||QK^T||}</script></li><li><p>温度参数：$d_k$ 换成常数</p></li><li><p>预归一化</p><script type="math/tex; mode=display"> Q'=\frac{Q}{||Q||}, K'=\frac{K}{||K||}, V'=V</script></li><li><p>长序列放缩，长序列导致 Attention 分数范围增大，Softmax 失效</p><script type="math/tex; mode=display"> softmax(\frac{QK^T}{\sqrt{d_k} \cdot \sqrt{T}})V\\</script></li></ol><h3 id="Self-Attention-代码手撕"><a href="#Self-Attention-代码手撕" class="headerlink" title="Self-Attention 代码手撕"></a>Self-Attention 代码手撕</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim, dim_k=<span class="hljs-number">512</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.norm_factor = <span class="hljs-number">1</span> / math.sqrt(dim_k)<br>        <span class="hljs-variable language_">self</span>.q = nn.Linear(input_dim, dim_k)<br>        <span class="hljs-variable language_">self</span>.k = nn.Linear(input_dim, dim_k)<br>        <span class="hljs-variable language_">self</span>.v = nn.Linear(input_dim, dim_k)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        x.shape: [B, T, D]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        Q, K, V = <span class="hljs-variable language_">self</span>.q(x), <span class="hljs-variable language_">self</span>.k(x), <span class="hljs-variable language_">self</span>.v(x)<br>        <span class="hljs-comment"># torch.bmm 输入为 3 维矩阵, 批量相乘, 速度快</span><br>        <span class="hljs-comment"># torch.matmul 输入可为多种矩阵, 更灵活</span><br>        score = torch.bmm(Q, K.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) * <span class="hljs-variable language_">self</span>.norm_factor<br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            score += mask * -<span class="hljs-number">1e9</span><br>        <span class="hljs-keyword">return</span> torch.bmm(torch.softmax(score, dim=-<span class="hljs-number">1</span>), V)<br></code></pre></td></tr></table></figure><h2 id="多头自注意力"><a href="#多头自注意力" class="headerlink" title="多头自注意力"></a>多头自注意力</h2><h3 id="为什么要多头？"><a href="#为什么要多头？" class="headerlink" title="为什么要多头？"></a>为什么要多头？</h3><p>模型只能学习到一个层面的注意力模式，不能捕捉到输入序列中复杂的多样性关系。仅通过单个头来表示查询、键和值的投影，会限制模型的表达能力。<br>多头注意力的优势包括：</p><ul><li><strong>捕捉不同的上下文信息</strong>：每个注意力头可以专注于不同的上下文信息或关系。例如，一个头可以专注于捕捉远距离词语之间的关系，而另一个头可以专注于局部词语之间的关系。</li><li><strong>提高模型的表达能力</strong>：通过并行计算多个注意力分布，模型能够从多个角度理解同一输入，从而获得<strong>更丰富的语义信息</strong>。</li><li><strong>提升模型的灵活性和鲁棒性</strong>：多头注意力使得模型能够在多个子空间中进行学习，从而减少单一注意力头可能带来的信息损失。</li></ul><p>举例：“The quick brown fox jumped over the lazy dog”。我们希望 Transformer 模型能够理解以下关系：</p><ul><li><strong>主谓关系</strong>：“fox” 和 “jumped”</li><li><strong>定语关系</strong>：“quick” 修饰 “fox”</li><li><strong>位置关系</strong>：“over” 与 “jumped”</li></ul><p>对模型来说，不同的头用来学习单词之间的不同关系，比如 “jump” 的头 1 用来学习与”fox”的主谓关系，头 2 用来学习与”over”的位置关系。</p><h3 id="代码手撕"><a href="#代码手撕" class="headerlink" title="代码手撕"></a>代码手撕</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiheadAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim, dim_k, num_head=<span class="hljs-number">8</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">assert</span> dim_k % num_head == <span class="hljs-number">0</span><br>        <span class="hljs-variable language_">self</span>.dk = dim_k // num_head<br>        <span class="hljs-variable language_">self</span>.head = num_head<br>        <span class="hljs-comment"># dk 缩减后放缩因子也要改变</span><br>        <span class="hljs-variable language_">self</span>.norm_factor = <span class="hljs-number">1</span> / math.sqrt(<span class="hljs-variable language_">self</span>.dk)<br>        <span class="hljs-variable language_">self</span>.q = nn.Linear(input_dim, dim_k)<br>        <span class="hljs-variable language_">self</span>.k = nn.Linear(input_dim, dim_k)<br>        <span class="hljs-variable language_">self</span>.v = nn.Linear(input_dim, dim_k)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        x.shape: [B, T, D]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        batch, seqlen, _ = x.shape<br>        Q, K, V = <span class="hljs-variable language_">self</span>.q(x), <span class="hljs-variable language_">self</span>.k(x), <span class="hljs-variable language_">self</span>.v(x)<br>        <span class="hljs-comment"># (B, T, D)</span><br>        Q, K, V = (<br>            Q.reshape(batch, seqlen, -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.dk).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            K.reshape(batch, seqlen, -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.dk).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            V.reshape(batch, seqlen, -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.dk).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>        )<br>        <span class="hljs-comment"># (B, H, T, dk)</span><br><br>        score = torch.matmul(Q, K.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) * <span class="hljs-variable language_">self</span>.norm_factor<br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            score += mask * -<span class="hljs-number">1e9</span><br>        output = torch.matmul(torch.softmax(score, dim=-<span class="hljs-number">1</span>), V)<br>        output = output.reshape(batch, seqlen, -<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法杂记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法</tag>
      
      <tag>基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deeplearning 基础</title>
    <link href="/deeplearning/basic/"/>
    <url>/deeplearning/basic/</url>
    
    <content type="html"><![CDATA[<h2 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h2><h3 id="1-sigmoid"><a href="#1-sigmoid" class="headerlink" title="1. sigmoid:"></a>1. sigmoid:</h3><script type="math/tex; mode=display">\sigma (x) = \frac{1}{1+e^{-x}}</script><p><strong>优点：</strong> 平滑且连续，适合二分类任务，具有明确的生物意义<br><strong>缺点：</strong> 输入绝对值过大时有梯度消失问题，输出大多集中在 0 和 1，可能导致网络的权重更新不平衡。指数运算效率较慢</p><h3 id="2-Tanh"><a href="#2-Tanh" class="headerlink" title="2. Tanh"></a>2. Tanh</h3><script type="math/tex; mode=display">Tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}</script><p><strong>优点：</strong> 输出均值为 0，收敛更快<br><strong>缺点：</strong> 仍有梯度消失问题，计算慢</p><h3 id="3-ReLU"><a href="#3-ReLU" class="headerlink" title="3. ReLU"></a>3. ReLU</h3><script type="math/tex; mode=display">ReLU(x) = max(x, 0)</script><p><strong>优点：</strong> 计算简单且高效，无梯度消失问题。具有稀疏性，能提高训练效率<br><strong>缺点：</strong> 对于 x&lt;0，梯度恒为 0，神经元可能“死亡”，不再更新。输出非对称：输出范围是 $[0,+\infty)$，可能导致权重更新不平衡</p><h3 id="4-Softmax"><a href="#4-Softmax" class="headerlink" title="4. Softmax"></a>4. Softmax</h3><script type="math/tex; mode=display">f_i(x)=\frac{e^{x_i}}{\sum_j e^{x_i}}</script><p><strong>优点：</strong> 输出为概率分布，适合<strong>多分类任务</strong>。强化高分输出，抑制低分输出，具有良好的区分性。<br><strong>缺点：</strong> 对异常值敏感，可能会梯度消失</p><blockquote><p>通常在实际应用中，会将 softmax 的值减去最大值，目的是避免数值溢出。指数运算 e 当数值过大时会溢出。减去最大值不会影响最终结果，并且可以避免数值溢出</p></blockquote><h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p>过拟合指，模型在训练集上的效果远远大于测试集/验证集上的效果</p><h3 id="缓解方法"><a href="#缓解方法" class="headerlink" title="缓解方法"></a>缓解方法</h3><ol><li>数据的角度：数据增强：图像数据就裁剪翻转等等、文本数据就是同义词替换</li><li>正则化：L1/L2 正则化，在损失函数添加模型的参数，L2 是平方和，L1 是绝对值和。L2 正则化通常能避免权重过大，L1 正则化可以产生稀疏的模型（即很多权重为0，可以实现特征选择，计算资源需求小）。</li><li>dropout：随机对一些参数置 0</li><li>早停：在验证集上性能下降就早停</li><li>噪声注入</li></ol><h3 id="正则化有哪些？为什么要加入正则化"><a href="#正则化有哪些？为什么要加入正则化" class="headerlink" title="正则化有哪些？为什么要加入正则化"></a>正则化有哪些？为什么要加入正则化</h3><p><strong>正则化的目的</strong>是通过限制模型的复杂度，防止过拟合</p><ul><li>L1 正则化： $L=Loss(y,\hat y)+ \lambda \sum_i |w_i|$，在损失函数中增加所有权重绝对值的惩罚项，会将某些权重 $w_i$ 收缩为 0，起到变量选择的作用。适用于<strong>特征稀疏的场景</strong>，容易导致某些特征被完全忽略。</li><li>L2 正则化：$L=Loss(y, \hat y)+ \lambda \sum_i w_i^2 $，不会让权重完全为 0，而是倾向于将权重的值缩小。更适合多特征问题。</li><li>Dropout：在每个训练批次中，以一定概率随机“屏蔽”一部分神经元，使其不参与前向和反向传播。防止神经网络中过度依赖某些特定的神经元</li></ul><h2 id="LayerNorm-amp-BatchNorm"><a href="#LayerNorm-amp-BatchNorm" class="headerlink" title="LayerNorm \&amp; BatchNorm"></a>LayerNorm \&amp; BatchNorm</h2><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h3><p>MSE: Mean Squared Error<br>均方误差是指参数估计值与参数真值之差平方的期望值;<br>MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。</p><script type="math/tex; mode=display">\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2</script><h3 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h3><p>RMSE: Root Mean Squared Error<br>均方根误差:均方根误差是均方误差的算术平方根</p><script type="math/tex; mode=display">\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}</script><h3 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h3><p>MAE :Mean Absolute Error<br>平均绝对误差是绝对误差的平均值;<br>平均绝对误差能更好地反映预测值误差的实际情况.</p><script type="math/tex; mode=display">\text{MAE} = \frac{1}{N} \sum_{i=1}^N |(y_i - \hat{y}_i)|</script><h3 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h3><p>熵：信息量关于概率分布 $P$ 的期望，$H(x)=-\sum_{i=0}^{N} P(X=x_i)logP(X=x_i)$</p><blockquote><p>那些接近确定性的分布（输出几乎可以确定）具有较低的熵，那些接近均匀分布的概率分布具有较高的熵。故用交叉熵损失可让分类的分布更为确定/准确</p></blockquote><p>单标签分类：$Loss=-\sum_{i=0}^{N}y_i \log \hat y_i$，其中 $y_i$ 为真实分布，$\hat y_i$ 为预测分布(输出的概率).</p><blockquote><p>用一个例子来说明，在手写数字识别任务中，如果样本是数字“5”，那么真实分布应该为：[ 0, 0, 0, 0, 0, 1, 0, 0, 0, 0 ]，网络输出的分布为：[ 0.1, 0.1, 0, 0, 0, 0.7, 0, 0.1, 0, 0 ]</p></blockquote><p>多标签分类(二元交叉熵损失)：$Loss=\frac{1}{n} \sum -y \log \hat y - (1-y) \log(1-\hat y)$</p><blockquote><p>例如，图中有米饭和一些菜品，假设当前的多标签分类任务有三个标签：米饭(A)、南瓜(B)、青菜(C). 很明显，图中是没有青菜的，它的真实分布应该为: [ 1, 1, 0 ]. 假设网络输出的概率分布为: [ 0.8, 0.9, 0.1 ]</p><script type="math/tex; mode=display">\begin{aligned}Loss_A &= -1 \log 0.8 - (1-1) \log (1-0.8)\\\\Loss_B &= -1 \log 0.9 - (1-1) \log (1-0.9)\\\\Loss_C &= -0 \log 0.1 - (1-0) \log (1-0.1)\\\\Loss_{all} &= Loss_A + Loss_B + Loss_C\\\\\end{aligned}</script></blockquote><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>函数：$z=wX+b$</p><p>模型输出经过 sigmoid：$\hat y =\sigma(z)=\frac{1}{1+e^{-z}}$</p><p>用二元交叉熵损失：$L=\frac{1}{n} \sum -y \log \hat y - (1-y) \log(1-\hat y)$</p><p>梯度推导：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial w}=\frac{1}{n} X^T (\hat y - y)\\\\\frac{\partial L}{\partial b}=\frac{1}{n} \sum_{i=1}^n (\hat y_i - y_i)\end{aligned}</script><p>梯度更新：$w^{(r+1)}=w^r-\alpha \frac{\partial L}{\partial w}$, $b^{(r+1)}=b^r-\alpha \frac{\partial L}{\partial b}$</p>]]></content>
    
    
    <categories>
      
      <category>算法杂记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>算法</tag>
      
      <tag>基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开篇首记</title>
    <link href="/undefined/"/>
    <url>/undefined/</url>
    
    <content type="html"><![CDATA[<p>欢迎来到我的博客！此网站主要用于记录博主的一系列学习心得，希望其中的内容对您有用！</p><p>若笔记中有任何错误，欢迎指正，也欢迎一起学习交流。邮箱: kent_guok@qq.com</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
